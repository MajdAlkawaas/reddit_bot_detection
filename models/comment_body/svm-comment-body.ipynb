{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "60% training, 20% validation, and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000 2000 3000 4000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 19000 20000 21000 22000 23000 24000 25000 26000 27000 28000 29000 30000 31000 32000 33000 34000 35000 36000 37000 38000 39000 40000 41000 42000 "
     ]
    }
   ],
   "source": [
    "# TODO: Move these two functions to a shared file for helper functions\n",
    "def unPickleData(filename):\n",
    "  with open(filename,'rb') as f: arr = pickle.load(f)\n",
    "  return arr\n",
    "\n",
    "def getDataPath(dirname,filename):\n",
    "    cwd = os.getcwd()\n",
    "    return os.path.join(cwd, os.path.join(\"processed_data\", dirname,filename))\n",
    "\n",
    "X_data = unPickleData(getDataPath(\"comments\",\"X_comments.pkl\"))\n",
    "y_data = unPickleData(getDataPath(\"comments\",\"Y_comments.pkl\"))\n",
    "\n",
    "# reducing amount of features\n",
    "X, y = [], []\n",
    "Class0_max_count = 38000\n",
    "for i in range(len(X_data)):\n",
    "    if y_data[i] == 0: \n",
    "        if Class0_max_count > 0:\n",
    "            # print(i)\n",
    "            X.append(X_data[i])\n",
    "            y.append(y_data[i])\n",
    "            if i%1000 == 0: print(i, end=\" \")\n",
    "            Class0_max_count -= 1 \n",
    "    else:\n",
    "        X.append(X_data[i])\n",
    "        y.append(y_data[i])\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.4, random_state=15, stratify=y)\n",
    "X_val,   X_test, y_val,   y_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state=15, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "model_lin = SVC(kernel=\"linear\", verbose=True)\n",
    "# model_lin.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model_lin, \n",
    "                                                                    X_train, \n",
    "                                                                    y_train, \n",
    "                                                                    scoring='accuracy', \n",
    "                                                                    cv=5, \n",
    "                                                                    return_times=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#D22B2B\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#ffcdcc\")\n",
    "\n",
    "# plt.ylim([0, 1])\n",
    "plt.xlim([0, train_sizes[-1]])\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = SVC(kernel=\"poly\", verbose=True)\n",
    "# model_poly.fit(X_train,y_train)\n",
    "\n",
    "train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model_lin, \n",
    "                                                                    X_train, \n",
    "                                                                    y_train, \n",
    "                                                                    scoring='accuracy', \n",
    "                                                                    cv=5, \n",
    "                                                                    return_times=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.subplots(1, figsize=(10,10))\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#D22B2B\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#ffcdcc\")\n",
    "\n",
    "# plt.ylim([0, 1])\n",
    "plt.xlim([0, train_sizes[-1]])\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\"C\":np.logspace(-3,3,100)}\n",
    "\n",
    "search = GridSearchCV(model_lin,grid, cv=10)\n",
    "search.fit(X_val,y_val)\n",
    "\n",
    "print(\"Tuned hpyerparameters :(best parameters) \",search.best_params_)\n",
    "print(\"Accuracy :\",search.best_score_)\n",
    "\n",
    "best_model_lin = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'C':np.logspace(-3,3,10), 'degree':[1,2,3,4,5], 'gamma':['scale', 'auto'], 'coef0':np.logspace(0,1,10)}\n",
    "\n",
    "search = GridSearchCV(model_poly,grid,cv=10)\n",
    "search.fit(X_val, y_val)\n",
    "\n",
    "print(\"Tuned hpyerparameters :(best parameters) \",search.best_params_)\n",
    "print(\"Accuracy :\",search.best_score_)\n",
    "\n",
    "best_model_poly = search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model_lin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10c0c00d8f99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_model_lin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model_lin' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = best_model_lin.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test,y_pred))\n",
    "print(\"Accuracy: \", sklearn.metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92       229\n",
      "           1       0.82      0.29      0.43        48\n",
      "\n",
      "    accuracy                           0.87       277\n",
      "   macro avg       0.85      0.64      0.68       277\n",
      "weighted avg       0.86      0.87      0.84       277\n",
      "\n",
      "Accuracy:  0.8664259927797834\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model_poly.predict(X_test)\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_test,y_pred))\n",
    "print(\"Accuracy: \", sklearn.metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including the validation set\n",
    "Because of the lack of representation of the class \"bot\", especially in the testing data, the model reflects poor results when it comes to the recall and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       459\n",
      "           1       0.93      0.43      0.58        94\n",
      "\n",
      "    accuracy                           0.90       553\n",
      "   macro avg       0.91      0.71      0.76       553\n",
      "weighted avg       0.90      0.90      0.88       553\n",
      "\n",
      "Accuracy:  0.8969258589511754\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model_lin.predict(np.concatenate((X_test,X_val))\n",
    ")\n",
    "print(sklearn.metrics.classification_report(np.concatenate((y_test,y_val)),y_pred))\n",
    "print(\"Accuracy: \", sklearn.metrics.accuracy_score(np.concatenate((y_test,y_val)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95       459\n",
      "           1       0.92      0.50      0.65        94\n",
      "\n",
      "    accuracy                           0.91       553\n",
      "   macro avg       0.91      0.75      0.80       553\n",
      "weighted avg       0.91      0.91      0.90       553\n",
      "\n",
      "Accuracy:  0.9077757685352622\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model_poly.predict(np.concatenate((X_test,X_val))\n",
    ")\n",
    "print(sklearn.metrics.classification_report(np.concatenate((y_test,y_val)),y_pred))\n",
    "print(\"Accuracy: \", sklearn.metrics.accuracy_score(np.concatenate((y_test,y_val)), y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f8db473d6762dfb6f9dcd7989df9714501dfc9335e8a68c24282034a9bd8d9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
